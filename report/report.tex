\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[final, nonatbib]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{cite}
\usepackage{graphicx}
\usepackage{amsmath}
\newcommand{\aaron}[2]{{\color{orange}\bfseries [aaron: #1]}}
\title{Evaluating the Robustness of Synthetic Graph Data Against Poisoning Attacks}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Aaron Zachariah \\
  Department of Computer Science\\
  Rensselaer Polytechnic Institute\\
  Troy, NY 12180 \\
  \texttt{zachaa2@rpi.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph.
\end{abstract}


\section{Introduction}
\label{Intro}

In this section, I introduce the problem statement for this work, as well as some related key ideas. I establish motivation for this work, and convey key hypotheses this work aims to evaluate. 

\subsection{Graph Data}

Graph data is an important and ubiquitous method of representing relationships between entities. The beauty of graphs is that they are an incredibly flexible way to represent relational data. As a result, graphs have application in all types of fields, including social networks \cite{socialnetworks}, biological networks \cite{Girvan_2002}, and voting networks \cite{votingnetworks}. Real world graph data often contains a trove of information, in regards to both local and global characteristics. These graphs can also be used for various tasks, including link prediction, node classification, and community detection, among many others. Naturally, machine learning researchers have devoted considerable time and effort into exploring the applications of machine learning methods for graph data, for the sake of extracting information from a graph. Some landmark successes in this realm is with Node2Vec \cite{grover2016node2vec}, the graph neural network \cite{GNNModel}, and matrix factorization \cite{matrixfactor}. These methods are for graph representation learning. As the name implies, the goal is to learn a low dimensional embedding that represents the graph's key features. The effectiveness of machine learning methods on downstream tasks depends heavily on the quality of the learned embeddings. 

\subsection{Representation Learning}

Representation learning on graphs is a challenging task, because graphs are non-Euclidean and do not have a fixed structure. Moreover, the dimensional of the target embeddings plays a key role, as there is a trade-off between high dimension and low dimension embeddings \cite{DBLP:journals/corr/abs-1909-00958}. Higher dimension embeddings preserve more graphical information, but at the cost of storage and computation. Lower dimension embeddings can potentially remove noise and are more space efficient, but they may lose some critical information about the graph. 

One of the most dominant approaches to graph representation learning is Graph Neural Networks (GNNs). GNNs take as input the graph structure, typically represented as an adjacency matrix. Then they learn embeddings from the graph structure to obtain a useful representation of the original graph structure. Like other deep learning methods, GNNs tend to over fit on the given graph structure \cite{bechlerspeicher2024graph}.

\subsection{Synthetic Graph Generation}

One of the effective ways to tackle representation learning on graphs is through applying synthetic graph generation. Synthetic graph generation is the process of algorithmic creation of graph data, instead of through real world data collection. In the context of representation learning, synthetic graph generation methods can be used for data augmentation. Augmentation of data is a particularly useful technique because it improves the generalization of deep learning models, which can prevent over fitting. In fact data augmentation approaches have shown to be useful in graph classification \cite{gmixup} and in class-imbalanced node classification \cite{graphmixup} settings.  

Synthetic graph generation also had the added benefit of being more practical to source than real world graph data. Real data requires observation and data collection, which is often a time consuming and expensive process. Synthetic graph generation on the other hand, requires implementing and executing an algorithm, which is more cost effective in terms of both time and money. In many cases, generator algorithms require some real data to learn a distribution. However because these algorithms can scale better than real data sourcing, algorithmic generation will still be more effective since it will be able to generate a large synthetic graph with a similar distribution, faster than it would take for researchers to observe and collect the data themselves. Thus, there is significant monetary and efficiency incentives for applying synthetic graph generation algorithms. 

\subsection{Poisoning Attacks}

As mentioned, graphs are a flexible way to model real world relations. However, often they are created from observation of accessible and easy to manipulate data, like ego networks, computer networks or web page networks. Since this data is sourced from the public domain, they are vulnerable to adversarial poisoning attacks. Poisoning attacks are adversarial attacks that are applied to a training dataset before it is used to train a model, with the goal of decreasing the model's effectiveness. In the context of graph representation learning, this means reducing the quality of the learned embeddings. Even though poisoning attacks involve manipulating the training set, in many cases an adversary may not need white-box access to do so. Consider a situation where a system supporting a public service has a bunch of users, and the administrators would like to support link prediction for a recommend service. To make this work, they would need need to make a network of their users and implement a link prediction algorithm. To degrade the performance of a downstream task such as link prediction, an adversary need only make accounts to poison the network. Such situations are perfectly feasible in a real world setting. 

\subsection{Main Goals}

This project aims to explore the robustness of synthetic graph data against graph poisoning attacks. The intuition is that synthetic data is easier to scale and may be more generalized. Thus, small perturbations made from a poisoning attack will have a reduced adverse effect on the learned embeddings. Existing work has shown that other defense mechanisms may not be useful in practice because they may drastically harm the generalization performance, or may simply be attack specific \cite{liu2023friendly}. Synthetic generation is extensible to any dataset of any type, and retains the generalization capabilities of the trained model. Thus, it may serve as a desirable way to defend, or at least mitigate the adverse effects of poisoning attacks. It may also be the case that applying synthetic algorithms on poisoned data only magnifies the effects. Previous work also notes that even small augmentations to the training data, such as altering the presence of nodes/edges, can result in a significant performance drop. Another feasible outcome is that adversarial perturbations compounded with data augmentation changes the graph structure too much, such that the learned embeddings are no longer useful for any downstream tasks. This work aims to empirically evaluate the robustness of synthetic data against poisoning attacks, and determine which hypothesized outcome is reality. 

\section{Background}
\label{background}

A graph $G$ is typically defined by $G = (V, E)$, where $V=\{v_1, ..., v_n\}$ is the set of $n$ vertices (or nodes) and $E=\{(u, v)\ |\ u, v \in V\}$ is the set of edges. Typically, we denote $n = |V|$ and $m = |E|$ as the sizes of the vertex set and edge set respoectively. Vertices and edges fully define a graph, but they are also typically represented in matrix form as an adjacency matrix. For a graph with $n$ vertices, an adjacency matrix $A \in\mathbb{R}^{n\times n}$, will have a 1 at element $A_{ij}$ is an edge exists between vertex $i$ and vertex $j$, and a 0 otherwise. Graphs may also have a vertex feature matrix, $\emph{X} \in \mathbb{R}^{n\times d}$, where $d$ is the dimension of the feature vector for some vertex $v_i \in V$. In the case where the graph is undirected, the adjacency matrix will be symmetric i.e., $A_{ij} = A_{ji}$. In many works, including this one, a directed graph will be used and will be converted to the undirected  matrix representation $A_{sym}$ in the following manner. 
\[
A_{sym} = A + A^T\ |\ if A_{ij} > 1: A_{ij} = 1\ \forall\ i,j \in \{1...n\}
\]

The most commonly studied class of graph are simple graphs, which are graphs that have no multi edges or self loops. For graphs with no multi edges, the adjacency matrix $A$ will be a binary matrix, meaning all entries will be either 0 or 1, indicating the presence or absence of an edge. For graphs with no self loops, this means that the diagonal will be 0, i.e., $A_{ii}=0\ \forall i \in \{1...n\}$. This indicates that no edge will have both endpoints on the same vertex. This work will consider only methods and experiments on simple graphs.

\begin{figure}[h]
\centering
\includegraphics[scale=1.0]{figures/SimpleGraph.png}
\caption{Simple vs. Non-Simple Graphs from \cite{simplegraphweisstein}}
\label{SimpleGraph}
\end{figure}

\subsection{Algorithmic Graph Creation}

Given the established formulation of a graph, we can define the problem of algorithmic graph generation as follows. Given a graph $G$ or a dataset of graphs $D_G$, with the underlying distribution $p(G)$, the goal is to construct a new graph $G_{new}$ that has a similar distribution. Obtaining the distribution $p(G)$ of a graph or set of graphs can be done in two main ways: \begin{enumerate}
  \setlength\itemsep{0.1em}
  \item \emph{Prescribed Graph Generation}: Taking a distribution or a set of parameters as input (could be mined from the original graph or user-generated)
  \item \emph{Deep Graph Generation}: Learning the distribution from the graph and sampling from it to construct $G_{new}$
\end{enumerate}

\paragraph{Prescribed Graph Generation} 

Some long-standing and well-researched methods for algorithmic graph generation are prescribed generative models. These prescribed generative methods are designed to encapsulate and recreate a defined set of statistical graph properties. Some earlier examples come from random graph models, such as Barabasi-Albert \cite{barabasi-albert} or Chung-Lu models \cite{chunglu}. These models are probabilistic models that operate under certain constraints. For example, the Chung-Lu model can generate a random undirected graph, but only with a certain provided expected degree distribution. 

\paragraph{Deep Graph Generation}

In recent years, methods involving deep neural networks (DNNs) have increased in popularity. This approach employs a deep neural network to learn how to obtain new samples from the distribution. This can be done by learning  estimated $p(G)$ first, and sampling from the estimated distribution, or by using an implicit strategy, which learns how to sample from the true distribution without actually estimating it. 

\subsection{Graph Poisoning Attacks}

Poisoning attack are simply training phase attacks where an adversary adds malicious samples to the training dataset. These adversarial attacks were first formalized in Netattack \cite{netattack}, and was separated into two paradigms in TDGIA \cite{TDGIA}. The two main paradigms for training time adversarial attacks are: (1) graph modification attacks (GMA) and (2) graph injection attacks (GIA). 

\paragraph{Graph Modification Attacks}

Recall the graph $G$ with adjacency matrix $A$ and feature matrix $X$. Let $\mathcal{M}:G\rightarrow \{1,2,...,C\}^N$ be a model that predicts labels for all $n$ vertices in $G$. We will denote the predictions as $\mathcal{M}(G)$. The goal of the GMA paradigm is to minimize the number of correct predictions made by $\mathcal{M}$ on the set of target vertices $\mathcal{T}$ by perturbing the original graph $G$:

\begin{equation}
\label{GMA}
\begin{split}
& \min_{G'}|\{\mathcal{M}(G')_i=y_i, i\in\mathcal{T}\}| \\
s.t.\ & G'=(A', \ X'),f_{\Delta_{A}}(A'-A)+f_{\Delta_{X}}(X'-X)\leq\Delta
\end{split}    
\end{equation}

where $G'$ is the perturbed graph, $y_i$ is the ground truth label for vertex $i$, and $f_{\Delta_{\mathbf{A}}}$ and $f_{\Delta_{\mathbf{F}}}$ are functions that measure the amount of perturbation. The $\Delta$ bound constrains the adversary to be able to perturb the graph only by a certain amount. 

\paragraph{Graph Injection Attacks}
GMA introduces perturbations on $G$'s structure and features. GIA on the other hand, will preserve the original graph's vertices and corresponding features, and simply inject $N_I$ new vertices into the graph. The objective of GIA is formalized as:
\begin{equation}
\label{GIA}
\begin{split}
& \min_{\mathbf{G}'} |\{\mathcal{M}(G')_i=y_i, i\in\mathcal{T}\}| \\
s.t.\ & G'=(A',X'), N_I \leq b, deg(v)_{v\in I}\leq d, ||\ X_I||\leq \Delta_X
\end{split}
\end{equation}

where $I$ is the set of injected vertices. $N_I$ is limited by a budget $b$, each vertex's degree is limited by a budget $d$ and the norm of the feature matrix of the injected vertices, $X_I$ is limited by $\Delta_X$.
\\
\\
One key difference between the two paradigms is that the GMA approach does not modify the shape of the adjacency matrix, but will apply perturbations to it. However, GIA will produce an adjacency matrix with a different change to the original, but will not perturb the original matrix at all. For ease of computation, I consider only GMA methods in this work, as their perturbations lie within the original adjacency matrix and not outside it. Retaining the shape of the adjacency matrix allows for a seamless transition through the experimental pipeline.

\section{Related Work}
\label{RelatedWork}

In this section, I introduce the relevant prior works as it related to the experiments done in this work. All the works discussed in this section are works that contribute to the subsequent experiments. 

\subsection{Deep Graph Generation}

As mentioned earlier, deep graph generation has seen increasing interest in recent years. Within the realm of deep generative models, there exist a variety of approaches, each using a different methodology and model architecture. For relevant state-of-the-art surveys of deep generative models for graph generation, see \cite{guo2022survey}, and \cite{zhu2022survey}. In general, they follow the following encoder $\rightarrow$ sampler$\rightarrow$ decoder pipeline as described in \cite{zhu2022survey}: 

\paragraph{Encoder} An encoding function $f_{\Theta}(\mathbf{z}\ |\ G)$ takes the discrete graph matrices and represent then as low dimensional continuous vector, following a stochastic distribution. Typically, probabilistic generative models are used as the encoder (e.g.,  variational graph neural networks).

\paragraph{Sampler} Graph generation requires sampling a latent representation from the learned distribution $z \sim p(z)$. The sampling strategy employed is either random sampling or controllable sampling. 

\paragraph{Decoder} The decoder takes the samples latent representations and converts them back into a graph structure. This is a challenging task, since graphs are discrete, non-Euclidean objects. The two generative strategies are \emph{sequential generation} and \emph{one-shot generation}.
\\
\\
Note that not every deep generative model need necessarily have all the components described above, but they follow the same approach generally. 

\subsubsection{NetGAN}

One such deep generative model is the NetGAN \cite{bojchevski2018netgan} model. 



\subsection{Adversarial Attacks on Graphs}



\subsection{Graph Neural Networks}


\aaron{Talk about the relevant papers used in the experiments here. A subsection for synthetix graph method and for attacks, and subsubsection for each individual method}{}

\section{Methods}
\label{Methods}
\aaron{Introduce the experimental design here, like the experiment pipeline (maybe a graphs? Also explain the downstream prediction tasks}{}

\section{Results}
\label{Results}
\aaron{put tables for all results and observations here}{}

\section{Discussion}
\label{Discussion}
\aaron{discuss the results and make the final key conclusions here}{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nocite{*}
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}