\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[final, nonatbib]{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{cite}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\usepackage{lipsum}
\usepackage{mwe}
\newcommand{\aaron}[2]{{\color{orange}\bfseries [aaron: #1]}}
\title{Evaluating the Robustness of Synthetic Graph Data Against Poisoning Attacks}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  Aaron Zachariah \\
  Department of Computer Science\\
  Rensselaer Polytechnic Institute\\
  Troy, NY 12180 \\
  \texttt{zachaa2@rpi.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}
  Synthetic graph generation is a useful alternate technique to generate graphs cheaper and at scale. Given a set of prescriptions or graph, they can produce a new synthetic graph with similar features. However, in real world settings graph data can be easy to attack, typically by poisoning the network. This work empirically evaluated the robustness of synthetic generators against adversarial graph attacks. In this work an experimental workflow is designed, and results on node classification and link predication is observed. Based on the observations, compounding synthetic generators with a pre-poisoned graph severely impacts the performance of GNNs on downstream tasks. As a result, solutions for future work are mentioned.
\end{abstract}


\section{Introduction}
\label{Intro}

In this section, I introduce the problem statement for this work, as well as some related key ideas. I establish motivation for this work, and convey key hypotheses this work aims to evaluate. 

\subsection{Graph Data}

Graph data is an important and ubiquitous method of representing relationships between entities. The beauty of graphs is that they are an incredibly flexible way to represent relational data. As a result, graphs have application in all types of fields, including social networks \cite{socialnetworks}, biological networks \cite{Girvan_2002}, and voting networks \cite{votingnetworks}. Real world graph data often contains a trove of information, in regards to both local and global characteristics. These graphs can also be used for various tasks, including link prediction, node classification, and community detection, among many others. Naturally, machine learning researchers have devoted considerable time and effort into exploring the applications of machine learning methods for graph data, for the sake of extracting information from a graph. Some landmark successes in this realm is with Node2Vec \cite{grover2016node2vec}, the graph neural network \cite{GNNModel}, and matrix factorization \cite{matrixfactor}. These methods are for graph representation learning. As the name implies, the goal is to learn a low dimensional embedding that represents the graph's key features. The effectiveness of machine learning methods on downstream tasks depends heavily on the quality of the learned embeddings. 

\subsection{Representation Learning}

Representation learning on graphs is a challenging task, because graphs are non-Euclidean and do not have a fixed structure. Moreover, the dimensional of the target embeddings plays a key role, as there is a trade-off between high dimension and low dimension embeddings \cite{DBLP:journals/corr/abs-1909-00958}. Higher dimension embeddings preserve more graphical information, but at the cost of storage and computation. Lower dimension embeddings can potentially remove noise and are more space efficient, but they may lose some critical information about the graph. 

One of the most dominant approaches to graph representation learning is Graph Neural Networks (GNNs). GNNs take as input the graph structure, typically represented as an adjacency matrix. Then they learn embeddings from the graph structure to obtain a useful representation of the original graph structure. Like other deep learning methods, GNNs tend to over fit on the given graph structure \cite{bechlerspeicher2024graph}.

\subsection{Synthetic Graph Generation}

One of the effective ways to tackle representation learning on graphs is through applying synthetic graph generation. Synthetic graph generation is the process of algorithmic creation of graph data, instead of through real world data collection. In the context of representation learning, synthetic graph generation methods can be used for data augmentation. Augmentation of data is a particularly useful technique because it improves the generalization of deep learning models, which can prevent over fitting. In fact data augmentation approaches have shown to be useful in graph classification \cite{gmixup} and in class-imbalanced node classification \cite{graphmixup} settings.  

Synthetic graph generation also had the added benefit of being more practical to source than real world graph data. Real data requires observation and data collection, which is often a time consuming and expensive process. Synthetic graph generation on the other hand, requires implementing and executing an algorithm, which is more cost effective in terms of both time and money. In many cases, generator algorithms require some real data to learn a distribution. However because these algorithms can scale better than real data sourcing, algorithmic generation will still be more effective since it will be able to generate a large synthetic graph with a similar distribution, faster than it would take for researchers to observe and collect the data themselves. Thus, there is significant monetary and efficiency incentives for applying synthetic graph generation algorithms. 

\subsection{Poisoning Attacks}

As mentioned, graphs are a flexible way to model real world relations. However, often they are created from observation of accessible and easy to manipulate data, like ego networks, computer networks or web page networks. Since this data is sourced from the public domain, they are vulnerable to adversarial poisoning attacks. Poisoning attacks are adversarial attacks that are applied to a training dataset before it is used to train a model, with the goal of decreasing the model's effectiveness. In the context of graph representation learning, this means reducing the quality of the learned embeddings. Even though poisoning attacks involve manipulating the training set, in many cases an adversary may not need white-box access to do so. Consider a situation where a system supporting a public service has a bunch of users, and the administrators would like to support link prediction for a recommend service. To make this work, they would need need to make a network of their users and implement a link prediction algorithm. To degrade the performance of a downstream task such as link prediction, an adversary need only make accounts to poison the network. Such situations are perfectly feasible in a real world setting. 

\subsection{Main Goals}

This project aims to explore the robustness of synthetic graph data against graph poisoning attacks. The intuition is that synthetic data is easier to scale and may be more generalized. Thus, small perturbations made from a poisoning attack will have a reduced adverse effect on the learned embeddings. Existing work has shown that other defense mechanisms may not be useful in practice because they may drastically harm the generalization performance, or may simply be attack specific \cite{liu2023friendly}. Synthetic generation is extensible to any dataset of any type, and retains the generalization capabilities of the trained model. Thus, it may serve as a desirable way to defend, or at least mitigate the adverse effects of poisoning attacks. It may also be the case that applying synthetic algorithms on poisoned data only magnifies the effects. Previous work also notes that even small augmentations to the training data, such as altering the presence of nodes/edges, can result in a significant performance drop. Another feasible outcome is that adversarial perturbations compounded with data augmentation changes the graph structure too much, such that the learned embeddings are no longer useful for any downstream tasks. This work aims to empirically evaluate the robustness of synthetic data against poisoning attacks, and determine which hypothesized outcome is reality. 

\section{Background}
\label{background}

In this section, I formulate the problem and establish key definitions and concepts that are related to the following experiments and discussion. 

\subsection{Graph Data}

A graph $G$ is typically defined by $G = (V, E)$, where $V=\{v_1, ..., v_n\}$ is the set of $n$ vertices (or nodes) and $E=\{(u, v)\ |\ u, v \in V\}$ is the set of edges. Typically, we denote $n = |V|$ and $m = |E|$ as the sizes of the vertex set and edge set respoectively. Vertices and edges fully define a graph, but they are also typically represented in matrix form as an adjacency matrix. For a graph with $n$ vertices, an adjacency matrix $A \in\mathbb{R}^{n\times n}$, will have a 1 at element $A_{ij}$ is an edge exists between vertex $i$ and vertex $j$, and a 0 otherwise. Graphs may also have a vertex feature matrix, $\emph{X} \in \mathbb{R}^{n\times d}$, where $d$ is the dimension of the feature vector for some vertex $v_i \in V$. In the case where the graph is undirected, the adjacency matrix will be symmetric i.e., $A_{ij} = A_{ji}$. In many works, including this one, a directed graph will be used and will be converted to the undirected  matrix representation $A_{sym}$ in the following manner. 
\[
A_{sym} = A + A^T\ |\ if A_{ij} > 1: A_{ij} = 1\ \forall\ i,j \in \{1...n\}
\]

The most commonly studied class of graph are simple graphs, which are graphs that have no multi edges or self loops. For graphs with no multi edges, the adjacency matrix $A$ will be a binary matrix, meaning all entries will be either 0 or 1, indicating the presence or absence of an edge. For graphs with no self loops, this means that the diagonal will be 0, i.e., $A_{ii}=0\ \forall i \in \{1...n\}$. This indicates that no edge will have both endpoints on the same vertex. This work will consider only methods and experiments on simple graphs.

\begin{figure}[h]
\centering
\includegraphics[scale=1.0]{figures/SimpleGraph.png}
\caption{Simple vs. Non-Simple Graphs from \cite{simplegraphweisstein}}
\label{SimpleGraph}
\end{figure}

\subsection{Algorithmic Graph Creation}

Given the established formulation of a graph, we can define the problem of algorithmic graph generation as follows. Given a graph $G$ or a dataset of graphs $D_G$, with the underlying distribution $p(G)$, the goal is to construct a new graph $G_{new}$ that has a similar distribution. Obtaining the distribution $p(G)$ of a graph or set of graphs can be done in two main ways: \begin{enumerate}
  \setlength\itemsep{0.1em}
  \item \emph{Prescribed Graph Generation}: Taking a distribution or a set of parameters as input (could be mined from the original graph or user-generated)
  \item \emph{Deep Graph Generation}: Learning the distribution from the graph and sampling from it to construct $G_{new}$
\end{enumerate}

\paragraph{Prescribed Graph Generation} 

Some long-standing and well-researched methods for algorithmic graph generation are prescribed generative models. These prescribed generative methods are designed to encapsulate and recreate a defined set of statistical graph properties. Some earlier examples come from random graph models, such as Barabasi-Albert \cite{barabasi-albert} or Chung-Lu models \cite{chunglu}. These models are probabilistic models that operate under certain constraints. For example, the Chung-Lu model can generate a random undirected graph, but only with a certain provided expected degree distribution. 

\paragraph{Deep Graph Generation}

In recent years, methods involving deep neural networks (DNNs) have increased in popularity. This approach employs a deep neural network to learn how to obtain new samples from the distribution. This can be done by learning  estimated $p(G)$ first, and sampling from the estimated distribution, or by using an implicit strategy, which learns how to sample from the true distribution without actually estimating it. 

\subsection{Graph Poisoning Attacks}

Poisoning attack are simply training phase attacks where an adversary adds malicious samples to the training dataset. These adversarial attacks were first formalized in Netattack \cite{netattack}, and was separated into two paradigms in TDGIA \cite{TDGIA}. The two main paradigms for training time adversarial attacks are: (1) graph modification attacks (GMA) and (2) graph injection attacks (GIA). 

\paragraph{Graph Modification Attacks}

Recall the graph $G$ with adjacency matrix $A$ and feature matrix $X$. Let $\mathcal{M}:G\rightarrow \{1,2,...,C\}^N$ be a model that predicts labels for all $n$ vertices in $G$. We will denote the predictions as $\mathcal{M}(G)$. The goal of the GMA paradigm is to minimize the number of correct predictions made by $\mathcal{M}$ on the set of target vertices $\mathcal{T}$ by perturbing the original graph $G$:

\begin{equation}
\label{GMA}
\begin{split}
& \min_{G'}|\{\mathcal{M}(G')_i=y_i, i\in\mathcal{T}\}| \\
s.t.\ & G'=(A', \ X'),f_{\Delta_{A}}(A'-A)+f_{\Delta_{X}}(X'-X)\leq\Delta
\end{split}    
\end{equation}

where $G'$ is the perturbed graph, $y_i$ is the ground truth label for vertex $i$, and $f_{\Delta_{\mathbf{A}}}$ and $f_{\Delta_{\mathbf{F}}}$ are functions that measure the amount of perturbation. The $\Delta$ bound constrains the adversary to be able to perturb the graph only by a certain amount. 

\paragraph{Graph Injection Attacks}
GMA introduces perturbations on $G$'s structure and features. GIA on the other hand, will preserve the original graph's vertices and corresponding features, and simply inject $N_I$ new vertices into the graph. The objective of GIA is formalized as:
\begin{equation}
\label{GIA}
\begin{split}
& \min_{\mathbf{G}'} |\{\mathcal{M}(G')_i=y_i, i\in\mathcal{T}\}| \\
s.t.\ & G'=(A',X'), N_I \leq b, deg(v)_{v\in I}\leq d, ||\ X_I||\leq \Delta_X
\end{split}
\end{equation}

where $I$ is the set of injected vertices. $N_I$ is limited by a budget $b$, each vertex's degree is limited by a budget $d$ and the norm of the feature matrix of the injected vertices, $X_I$ is limited by $\Delta_X$.
\\
\\
One key difference between the two paradigms is that the GMA approach does not modify the shape of the adjacency matrix, but will apply perturbations to it. However, GIA will produce an adjacency matrix with a different change to the original, but will not perturb the original matrix at all. For ease of computation, I consider only GMA methods in this work, as their perturbations lie within the original adjacency matrix and not outside it. Retaining the shape of the adjacency matrix allows for a seamless transition through the experimental pipeline.

\section{Related Work}
\label{RelatedWork}

In this section, I introduce the relevant prior works as it related to the experiments done in this work. All the works discussed in this section are works that contribute to the subsequent experiments. 

\subsection{Deep Graph Generation}

As mentioned earlier, deep graph generation has seen increasing interest in recent years. Within the realm of deep generative models, there exist a variety of approaches, each using a different methodology and model architecture. For relevant state-of-the-art surveys of deep generative models for graph generation, see \cite{guo2022survey}, and \cite{zhu2022survey}. In general, they follow the following encoder $\rightarrow$ sampler$\rightarrow$ decoder pipeline as described in \cite{zhu2022survey}: 

\paragraph{Encoder} An encoding function $f_{\Theta}(\mathbf{z}\ |\ G)$ takes the discrete graph matrices and represent then as low dimensional continuous vector, following a stochastic distribution. Typically, probabilistic generative models are used as the encoder (e.g.,  variational graph neural networks).

\paragraph{Sampler} Graph generation requires sampling a latent representation from the learned distribution $z \sim p(z)$. The sampling strategy employed is either random sampling or controllable sampling. 

\paragraph{Decoder} The decoder takes the samples latent representations and converts them back into a graph structure. This is a challenging task, since graphs are discrete, non-Euclidean objects. The two generative strategies are \emph{sequential generation} and \emph{one-shot generation}.
\\
\\
Note that not every deep generative model need necessarily have all the components described above, but they follow the same approach generally. 

\subsubsection{NetGAN}

One such deep generative model is the NetGAN \cite{bojchevski2018netgan} model. NetGAN is a Generative Adversarial Network model for graph data, that leverages random walks to learn the topology of a given graph. The model first sample a bunch of random walk, which become the training set. Like other traditional GAN architectures, there is a Generator and Discriminator. The Generator's goal is to generate synthetic random walks that are plausible on the input graph. The Discriminator's goal is the distinguish synthetic random walks from the real random walks that were sampled from the input graph. The Generator and Discriminator are trained end-to-end via back-propagation. 

At any point in the training phase, one can use the Generator to create a set of random walks, which can then be turned into an adjacency matrix of a resulting synthetic graph. This is done by first using the Generator and provided set of random walks to construct a score matrix $\mathbf{S}$ of transition counts. The transition counts are simply how often an edge appears in the given set of random walks. The score matrix can then be turned into a binary adjacency matrix by first symmetrizing $\mathbf{S}$ (i.e., $s_{ij}=s_{ji}=\max\{s_{ij},s_{ji}\}$) and employing a binarization strategy. 

\subsection{Adversarial Attacks on Graphs}

Adversarial attacks on graphs is a field of study that has gained increasing interest in recent years. The main goal of adversarial attacks is to perturb the graph in order to degrade the quality of learned embeddings. There are two key attack paradigms: \textbf{Unsupervised Attacks} and \textbf{Supervised Attacks}. The main difference between these paradigms is that supervised attacks require the labels, and unsupervised attacks do not. In this work, I experiment with both supervised and unsupervised attacks. 

\subsubsection{Random Attacks}

The simplest and most naive approach to graph adversarial attacks is random perturbations. The simple random attack involves randomly selecting $\mathcal{T}$ edges in the adjacency matrix to perturb.  This is akin to simply adding an element of random noise into the graph structure. While noise will certainly affect the learned embeddings, the random attack doesn't efficiently use the perturbation budget $\mathcal{T}$ effectively since not all edges are equally important to the overall graph structure. A simple optimization to the random attack is implemented in the form of DICE, which stands for 'delete internally, connect externally' (as described in \cite{zügner2024Metattack}). As the name implies, this method will randomly perturb edges but only if the edge removed is incident on two vertices of the same class, or only if the edge added is incident on two vertices of a different class. The intuition is that adding inter-class connections and removing intra-class connections will degrade a DNN's ability to learn a proper representation. 

\subsubsection{Optimization-Based Attacks}

PGD and MinMax \cite{xu2019topology} are optimization based topology attacks, where they first define the attacker loss, and leverage first-order optimization to develop the attack generation methods. The attacker loss can be modeled as the \emph{negative cross-entropy (CE) loss} between the true and predicted labels as shown in \cite{goodfellow2015explaining}. In the case of attacking a pre-defined GNN with a fixed $\mathbf{W}$, the optimization problem can be formulated as

\begin{equation}
    \min_{\mathbf{s}}\ \sum_{i \in \mathcal{V}} f_i(\mathbf{s}; \mathbf{W}, \mathbf{A}, \{\mathbf{x}_i\}, y_i)   
\end{equation}
\[
\text{subject to}\ \mathbf{1}^\top \mathbf{s} \leq \epsilon, \quad \mathbf{s} \in \{0, 1\}^{n}
\]

where $\mathbf{s}$ is a vector that consists of perturbation variables that describe the perturbations done on the adjacency matrix. This optimization problem is further relaxed into a continuous optimization problem for ease of optimization. The continuous optimization problem is solved by projected gradient descent, which is known as the PGD method. In the case where the GNN has re-trainable $\mathbf{W}$ the optimization problem becomes the following min-max problem. 

\begin{equation}
    \min_{\mathbf{1}^\top \mathbf{s} \leq \epsilon, \quad \mathbf{s} \in \{0, 1\}^{n}}\ 
    \max_{\mathbf{W}} \sum_{i \in \mathcal{V}} f_i(\mathbf{s}, \mathbf{W}; \mathbf{A}, \{\mathbf{x}_i\}, y_i)
\end{equation}

As with PGD, the above optimization problem is relaxed into a continuous optimization problem. The problem is solved by first-order alternating optimization \cite{minmaxopt}, where the inner max problem is solved by gradient ascent and the outer min problem is solved by PDG.

\subsubsection{Metattack}

Metattack \cite{zügner2024Metattack} is another proposed graph poisoning attack that works by optimizing the meta-gradient. This approach is motivated by meta-learning, or learning to learn. Meta learning aims to make machine learning models more efficient by learning suitable hyperparamters. The main idea behind this adversarial attack algorithm is to treat the adjacency matrix as a hyperparameter, and compute the gradient of the attacker's loss after training with respect to it:
\begin{equation}
    \nabla_{G}^{meta} := \nabla_{G}\mathcal{L}_{atk}(f_{\theta}\cdot(G))\ \ \ s.t. \ \ \
    \theta^{*} = \text{opt}_{\theta}(\mathcal{L}_{\text{train}}(f_{\theta}(G)))
\end{equation}

where $\text{opt}(\cdot)$ is a differentiable optimization function (e.g., gradient descent) and $\mathcal{L}_{\text{train}}$ is the training loss. The goal is to minimize the attacker loss $\mathcal{L}_{\text{atk}}$.

\subsection{Graph Neural Networks}

Graph Neural Networks (GNNs) are a popular way to do graph representation learning. Under the umbrella of GNNs, there are a variety of model architectures that learn embeddings for graph structures. One such approach that has resulted in state-of-the-art success is graph contrastive learning.

\subsubsection{Graph Contrastive Learning}

The goal of graph contrastive learning is to learn a GNN encoder $f(X, A)$ that takes the graph structure and features as input and produces low dimensional continuous embeddings. The graph contrastive learning framework is one where the model seeks to maximize the agreement between two representations of different views \cite{zhu2020grace}. To generate different views, stochastic graph augmentation is performed on the input. A contrastive objective function is then used to force the encoded embeddings for each vertex in the different views to agree with each other and can be discriminated from embeddings of different vertices \cite{Zhu_2021_GCA}. 

\section{Methods}
\label{Methods}

The ultimate goal of this work is to explore the robustness of synthetic graph data to poisoning attacks. In particular, will synthetic graph data improve generalization and reduce the adverse effects of poisoning attacks, or will the added noise result in learned embeddings that are not useful for any downstream tasks.

\subsection{Threat Model}

The threat model for these experiments are quite simple. The adversary is allowed to perturb the graph structure \emph{before} a GNN is trained. This is to model real world settings, where data is sourced from public or open sources, and the sanitization quality of data cannot be guaranteed. As mentioned, these experiments do not consider injection attacks, so all the attacks used do not modify the shape of the adjacency matrix, they will simply perturb entries in the matrix (edge addition/removal). The adversary is given a fixed budget $\mathcal{T}$ of edges to perturb. For the experiments, I consider 1\%/5\%/10\% poisoning ratios, where the budget $\mathcal{T}$ is the ratio of total edges in the graph. 

\subsection{Experimental Workflow}

\begin{figure}[h]
\centering
\includegraphics[scale=0.65]{figures/ExperimentProcedure.png}
\caption{Experimental Workflow}
\label{exp-prcedure}
\end{figure}

The basic experimental workflow is detailed in figure \ref{exp-prcedure}. We originally start with a clean graph, with the original adjacency matrix. Then, one of the attack algorithms from Section \ref{RelatedWork} will be applied to the graph structure, producing a poisoned adjacency matrix $A_{poison}$. At this point, the obtained $A_{poison}$ is meant to represent a real world dataset under an adversarial setting, where the data is not guaranteed to be clean. From here, we apply the synthetic graph generation algorithm, NetGAN, to produce a synthetic adjacency matrix from the poisoned one. Finally, we take this synthetic poisoned adjacency matrix, and use it on some down stream tasks. To implement the attack algorithms, I build off of an existing framework from \cite{Zhang_2022}, which uses a PyTorch based adversarial learning library for images and graphs, called DeepRobust \cite{li2020deeprobust}. To utilize NetGAN, I build off of an existing implementation of NetGAN in PyTorch provided in the following repository: \url{https://github.com/JunHao-Zhu/NetGAN-torch}.

\subsection{Downstream Tasks}

Since we are interested on the effects of synthetic generators on poisoned data, we will compare the results of two downstream tasks using the synthetic poisoned data and the non-synthetic poisoned data. The downstream tasks used in this work are node classification and link prediction. The \textbf{Dataset} used in these experiments is the Cora dataset, with 2708 vertices and 5429 edges. The split used is a 70\%/20\%/10\% train/val/test split. 

\paragraph{Node Classification} 
The Cora dataset contains 7 classes, and vertices in the dataset lie in one of those 7 classes. The procedure for node classification will be to first train a graph contrastive model, GCA \cite{Zhu_2021_GCA} on an adjacency matrix to learn node embeddings. Then, we train a simple logistic regression model on the learned embeddings and perform classification and evaluation. The evaluation metric will be classification accuracy. 

\paragraph{Link Prediction}
For link prediction, the goal is to train a model to predict the existence of an edge between two given vertices. Again, the GCA model is trained to get learned node embeddings for a given adjacency matrix. Then, these embeddings are projected into anew latent space via a 2-layer MLP, which then is trained for link prediction with negative sampling and margin loss, as seen in \cite{Zhang_2022}.

\section{Results}
\label{Results}

The results in figures \ref{fig:class1} and \ref{fig:class2} display the classification accuracy on Cora for the non-synthetic poisoned data and synthetic poisoned data respectively. As shown in the results, there is a large disparity between results obtained by only applying the attack algorithms compared to results from applying NetGAN after the attack algorithms. 

\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/class2.PNG}
        \caption{Node Classification results with Non-Synthetic Poisoned Data (acc)}
        \label{fig:class1}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/class1.PNG}
        \caption{Node Classification results with Synthetic Poisoned Data (acc)}
        \label{fig:class2}
    \end{minipage}
\end{figure}

The results in figures \ref{fig:link1} and \ref{fig:link2} show the link prediction auc score for non-synthetic poisoned data and synthetic poisoned data respectively. Note that an auc score of $0.5$ is akin to random guessing. Again, there is a clear disparity between the performance of the link prediction on the real poisoned adjacency matrix compared to the adjacency matrix produced by a generator. 

\begin{figure}[H]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/link2.PNG} 
        \caption{Link Pred. results with Non-Synthetic Poisoned Data (auc)}
        \label{fig:link1}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{figures/link1.PNG} 
        \caption{Link Pred. results with Synthetic Poisoned Data (auc)}
        \label{fig:link2}
    \end{minipage}
\end{figure}

\section{Discussion}
\label{Discussion}

Based on the results obtained from these experiments, it would seem that synthetic graph data is not robust to poisoning attacks at all. In fact, compounding poisoning attacks with a synthetic generator yields a graph structure that has little to no useful information anymore, as made apparent by the low accuracy and auc scores. The NetGAN paper \cite{bojchevski2018netgan} reports very reasonable auc scores for link prediction on the Cora dataset, but when that dataset has been poisoned, the results drop significantly. These results indicate that one should be wary when applying synthetic generators to graphs in order to scale them. If such networks are easy to attack, then the resulting synthetic graph may actually be of no use. 

Instead, it may be more practical to either defend against poisoning attacks by designing more robust algorithms or by implementing pr-processing steps to sanitize the dataset. A future line of would may be to apply some existing graph sanitation techniques such as \cite{zhu2023focusedcleaner} or \cite{xu2021graphSanitation} and observe if they can aide in preserving the utility of the synthetic graphs. 

\footnote{Code can be viewed at: \url{https://github.com/zachaa2/Security-And-Privacy-of-ML-Project}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nocite{*}
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}